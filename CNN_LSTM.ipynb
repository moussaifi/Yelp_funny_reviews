{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['yelp-dataset', 'balanced-reviews']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the \"../input/\" directory.\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n",
    "import re\n",
    "import string\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import os\n",
    "print(os.listdir(\"../input\"))\n",
    "\n",
    "# Any results you write to the current directory are saved as output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews_final = pd.read_csv(\"../input/balanced-reviews/balanced_reviews.csv\", index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_reviews = reviews_final[['funny','text', 'fun_bin']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>funny</th>\n",
       "      <th>text</th>\n",
       "      <th>fun_bin</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>7</td>\n",
       "      <td>I love chinese food and I love mexican food. What can go wrong? A couple of things. First things first, this place is more of a \"rice bowl\" kind of place. I thought it was going to be more diverse as far as the menu goes, but its mainly rice bowls you get with different kinds of meats. The ordering was a little confusing at first, but one of the employees helped us out and I got the 2-item bowl and got the jade chicken and hengrenade chicken with all rice(jerk). I also ordered a jade chicken quesadilla on the side.\\n\\nI'm gonna admit, this place looks kinda dirty. I don't think Arizona uses those health department letter grade system like California does, but if I were to just judge by how it looked inside, i'd give it a \"C\" grade lol. We waited for about 15 minutes or so and finally got our food. We took it to go and ate at our hotel room. \\n\\nMmmm... the food was just alright. The jade chicken was nothing special. It tasted like any generic chinese fast food orange chicken/sesame chicken variant. The hengrenade chicken, although was the less spicier version of the jerk chicken, was still pretty spicy for me. Just be warned the jerk chicken is super spicy. If you aren't sure, ask for a sample at the restaurant before ordering, but it was way too spicy for me. \\n\\nThe jade chicken quesadilla was decent, but nothing special. Just imagine orange chicken in between a tortilla and cheese. A friend of mine ordered a jade chicken burrito and we were confused when we pulled it out of the bag because it was literally the size of Mcdonald's apple pie. If you order the burrito, be warned that it's a burrito for gnomes and smurfs, but he said it was tasty. \\n\\nThey provide a snicker doodle sugar cookie for each meal and it was decent, again nothing special. \\n\\nNot gonna lie, the next day my stomach felt like a little mexican dude and chinese dude were wrestling and throwing molotov cocktails inside. I used the bathroom like 5 times. I don't recommend eating this place if you have a lot to do the next day.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>6</td>\n",
       "      <td>If you are looking for the best pierogies in Pittsburgh, this is your place. There are a few small tables outside but most of the business is carry out. Pierogies Plus wins Best Pierogies every year. Why? Because the owner is from Poland and she is making the real deal pierogies. The best part is that they are hand pinched by a group of older Polish and Hungarian women. \\nThe biggest seller is potato and cheese but they sell many flavors. They are like plump pillows of softness. You can buy them buy the dozen. You can get them cold to take home and freeze or warm and ready to eat. The warm ones are served with butter and onions.  It's definitely a comfort food. The best part is that they ship internationally. Yes, they are that good.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>5</td>\n",
       "      <td>So good! They didn't make it to 5 stars due to the prices are a bit high for the amount of food and the location is a bit unsavory. \\nThe decor and atmosphere was surprisingly nice, from the outside I expected to be more run down inside. The staff was very nice. We were surprised how empty the dining room was for a Friday evening.\\nWe got Vegetable Samosas to start then ordered Chicken Tikka Masala, Lamb Rogan Josh, rice and plain Naan. Our only complaint was the lamb could've been more tender but everything was flavorful and delicious. \\nI would definitely go again if given the chance.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>126</th>\n",
       "      <td>5</td>\n",
       "      <td>While the prices are a bit high for a make-your-own pizza, the taste makes up for it. I love going to Seventh Street market, sitting Not Just Coffee and having a drink while waiting for delicious fresh made pizza from Pure.  I've taken this to go as well as eaten inside the market, and I can say that the pizza doesn't do well reheated. So try to eat it fresh while there if possible.\\n\\nIf one of their specialty pizzas sounds good to you, go for it, as those are definitely a better deal for the amount of toppings you get for the money.  I wanted what I wanted, though, so I ended up with a medium, thin crust, regular crust pizza with jalapenos, pepperoncini, onions and feta.  It was pretty expensive at $2/topping = $20 med pizza. But it was delicious.\\n\\nThe arugula salad with goat cheese and lemon vinaigrette is to-die-for. I crave that dressing days later. So light and fresh but flavorful.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>246</th>\n",
       "      <td>5</td>\n",
       "      <td>OVERALL: The food isn't good (I explain below), but this place may still be worth locals' time (and more importantly money).  Let me explain...\\n\\nThere are not many \"old\" restaurants in this town. We don't seem to value/frequent/patronize places that have been around putting out food for a long time. I think we should. Even when the food isn't show stopping. Why? This place has tremendous character and charm. There's an \"Old Western Vegas\" feel to Bob Taylor's. Established in 1955, it's the oldest restaurant in Las Vegas. Its a throwback to a rugged, carnivorous cowboy culture that has existed in this town for decades. And still exists. I did appreciate the slice of Vegas kitch that Bob Taylor's offers. \\n\\nFOOD ISSUES: So with all that charm how could this place go wrong? This place could be great. It really should be great. But they are not putting enough care into the food. I ordered the rib eye and asked for it to be medium rare. I was worried about it being overcooked and figured if a mistake was made, I'd be in the medium range. My instincts were correct. But the steak was closer to well done. In total, three steaks at our table were seriously overcooked. In a steakhouse. With a man tasked with grilling the steaks. Sigh. The fourth steak, smoked prime rib, was cooked properly. But the prime rib is cooked ahead if time, right? How was I prepared for this overcooked piece of meat? How did this only occasional red-meat-eater suspect that my steak would not be treated with attentive care? \\n\\nI'll tell you. When we walked in there was a large grill at the front of the restaurant with a number if steaks cooking on it. But the chef was not watching the meat. He wasn't even in front of the grill. He was nowhere to be seen as we walked through the doors. And there were at least 4 steaks cooking when we arrived. So I figured that my steak would receive the same lack of attention.  \\n\\nI ordered a simple naked potato and a side salad to accompany my steak. Both were fine. But there is not much room to mess up a potato and iceberg, is there? People rave about the garlic bread and I think it's because the rest of the meal is so mediocre, that the cheesy bread becomes the highlight if the meal. It was just OK. The most inexperienced cook could make it at home with sourdough, butter, and three types of cheese. \\n\\nA few people in our party ordered the mushroom rice side dish and it was not good. After tasting it, I was grateful to have passed on this wet, mush. \\n\\nSERVICE: Our waitress was very attentive and responsive. She was more than willing to return the overcooked steaks. \\n\\nI won't be back, but I'm glad to have visited this historic spot.\\n\\nService: 4 stars\\n\\nKitch: 4 stars\\n\\nFood: 1 star</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     funny   ...   fun_bin\n",
       "17   7       ...    1     \n",
       "21   6       ...    1     \n",
       "62   5       ...    1     \n",
       "126  5       ...    1     \n",
       "246  5       ...    1     \n",
       "\n",
       "[5 rows x 3 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.set_option('display.max_colwidth', -1)\n",
    "df_reviews.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "#import nltk\n",
    "#from nltk.corpus import stopwords\n",
    "#from nltk.stem import SnowballStemmer\n",
    "\n",
    "from string import punctuation\n",
    "\n",
    "\n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "import os\n",
    "import spacy\n",
    "import string\n",
    "import re\n",
    "import numpy as np\n",
    "from spacy.symbols import ORTH\n",
    "from collections import Counter\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence \n",
    "#from spacymoji import Emoji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "re_br = re.compile(r'<\\s*br\\s*/?>', re.IGNORECASE)\n",
    "def sub_br(x): return re_br.sub(\"\\n\", x)\n",
    "\n",
    "#nlp = spacy.load(\"en\")\n",
    "spacy.load('en')\n",
    "spacy_stopwords = spacy.lang.en.stop_words.STOP_WORDS\n",
    "\n",
    "\n",
    "\n",
    "def clean_text(text):\n",
    "    ''' Pre process and convert texts to a list of words '''\n",
    "    text = str(text)\n",
    "    #text = text.lower()\n",
    "\n",
    "    # Clean the text\n",
    "   # text = re.sub(r\"[^A-Za-z0-9^,!.\\/'+-=\\(\\)]\", \" \", text) # keep punctuatuin, numnbers and letters\n",
    "    text = re.sub(r\"what's\", \"what is \", text)\n",
    "    text = re.sub(r\"\\'s\", \" \", text)\n",
    "    text = re.sub(r\"\\'ve\", \" have \", text)\n",
    "    text = re.sub(r\"can't\", \"cannot \", text)\n",
    "    text = re.sub(r\"n't\", \" not \", text)\n",
    "    text = re.sub(r\"i'm\", \"i am \", text)\n",
    "    text = re.sub(r\"\\'re\", \" are \", text)\n",
    "    text = re.sub(r\"\\'d\", \" would \", text)\n",
    "    text = re.sub(r\"\\'ll\", \" will \", text)\n",
    "    text = re.sub(r\",\", \" \", text)\n",
    "    text = re.sub(r\"\\.\", \" . \", text) #Add space to the dot\n",
    "    text = re.sub(r\"!\", \" ! \", text) #Add space to the exclamation sign\n",
    "    text = re.sub(r\":\", \" :\", text) #Add space before : sign\n",
    "    text = re.sub(r\";\", \" ;\", text) #Add space before ; sign\n",
    "    text = re.sub(r\"\\/\", \" \", text)\n",
    "    text = re.sub(r\"\\^\", \" ^ \", text)\n",
    "    text = re.sub(r\"\\+\", \" + \", text)\n",
    "    text = re.sub(r\"\\-\", \" - \", text)\n",
    "    text = re.sub(r\"\\=\", \" = \", text)\n",
    "    text = re.sub(r\"'\", \" \", text)\n",
    "    #text = re.sub(r\"(\\d+)(k)\", r\"\\g<1>000\", text)\n",
    "    #text = re.sub(r\":\", \" : \", text)\n",
    "    text = re.sub(r\" e g \", \" eg \", text)\n",
    "    text = re.sub(r\" b g \", \" bg \", text)\n",
    "    text = re.sub(r\" u s \", \" american \", text)\n",
    "    text = re.sub(r\"\\0s\", \"0\", text)\n",
    "    text = re.sub(r\" 9 11 \", \"911\", text)\n",
    "    text = re.sub(r\"e - mail\", \"email\", text)\n",
    "    text = re.sub(r\"j k\", \"jk\", text)\n",
    "    text = re.sub(r\"\\s{2,}\", \" \", text)\n",
    "    # find emojis\n",
    "    emoji_list = []\n",
    "    '''\n",
    "    for word in text.split():\n",
    "        if any(char in emoji.UNICODE_EMOJI for char in word):\n",
    "            emoji_list.append(word)\n",
    "    emoji_list'''\n",
    "    #text = text.split()\n",
    "\n",
    "    return text\n",
    "\n",
    "my_tok = spacy.load('en_core_web_sm')\n",
    "#emoji = Emoji(my_tok)\n",
    "#my_tok.add_pipe(emoji, first=True)\n",
    "def spacy_tok(x): return [tok.text for tok in my_tok.tokenizer(clean_text(x))]\n",
    "\n",
    "def remove_stop_words(tokens): return [tok for tok in tokens if tok not in spacy_stopwords]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Building a vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "counts = Counter()\n",
    "for sent in df_reviews['text']:\n",
    "    try:\n",
    "        counts.update(remove_stop_words(spacy_tok(sent)))\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vocabulary\n",
    "vocab2index = {\"\":0, \"UNK\":1}\n",
    "words = [\"\", \"UNK\"]\n",
    "for word in counts:\n",
    "    vocab2index[word] = len(words)\n",
    "    words.append(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "# WHat is the 99% quantile of  length of the sentence?\n",
    "df_reviews['len_text'] = df_reviews['text'].apply(lambda x: len(x.split()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "769.0"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_reviews['len_text'].quantile(0.99)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "496.0"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_reviews['len_text'].quantile(0.95)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"I'm soooo excited!!!!!This is 10000% the best place on earth:))))) 😃...\""
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"I'm soooo excited!!!!!This is 10000% the best place on earth:))))) 😃...\"\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I m soooo excited ! ! ! ! ! This is 10000% the best place on earth :))))) 😃 . . . '"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_text(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I',\n",
       " 'm',\n",
       " 'soooo',\n",
       " 'excited',\n",
       " '!',\n",
       " '!',\n",
       " '!',\n",
       " '!',\n",
       " '!',\n",
       " 'This',\n",
       " 'is',\n",
       " '10000',\n",
       " '%',\n",
       " 'the',\n",
       " 'best',\n",
       " 'place',\n",
       " 'on',\n",
       " 'earth',\n",
       " ':',\n",
       " ')',\n",
       " ')',\n",
       " ')',\n",
       " ')',\n",
       " ')',\n",
       " '😃',\n",
       " '.',\n",
       " '.',\n",
       " '.']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#spacy_tok(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I',\n",
       " 'm',\n",
       " 'soooo',\n",
       " 'excited',\n",
       " '!',\n",
       " '!',\n",
       " '!',\n",
       " '!',\n",
       " '!',\n",
       " 'This',\n",
       " '10000',\n",
       " '%',\n",
       " 'best',\n",
       " 'place',\n",
       " 'earth',\n",
       " ':',\n",
       " ')',\n",
       " ')',\n",
       " ')',\n",
       " ')',\n",
       " ')',\n",
       " '😃',\n",
       " '.',\n",
       " '.',\n",
       " '.']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#remove_stop_words(spacy_tok(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# note that spacy_tok takes a while run it just once\n",
    "def encode_sentence(sent, vocab2index, N=500, padding_start=True):\n",
    "    \"Encoding a sentence adding padding\"\n",
    "    x = remove_stop_words(spacy_tok(sent))\n",
    "    enc = np.zeros(N, dtype=np.int32)\n",
    "    enc1 = np.array([vocab2index.get(w, vocab2index[\"UNK\"]) for w in x])\n",
    "    l = min(N, len(enc1))\n",
    "    if padding_start:\n",
    "        enc[:l] = enc1[:l]\n",
    "    else:\n",
    "        enc[N-l:] = enc1[:l]\n",
    "    return enc, l"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting into train and validation sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(df_reviews['text'], df_reviews['fun_bin'], test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.reset_index(inplace=True, drop=True)\n",
    "X_valid.reset_index(inplace=True, drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.reset_index(inplace=True, drop=True)\n",
    "y_valid.reset_index(inplace=True, drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(120401,)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Writing a dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class YelpDataset(Dataset):\n",
    "    def __init__(self, df, y, N=400, padding_start=True):\n",
    "        self.df = df\n",
    "        self.X = [encode_sentence(sent, vocab2index, N, padding_start) for sent in self.df]\n",
    "        self.y = y\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        x, s = self.X[idx]\n",
    "        return x, s, self.y[idx]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds =  YelpDataset(X_train, y_train, padding_start=False)\n",
    "valid_ds =  YelpDataset(X_valid, y_valid, padding_start=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "neg=[]\n",
    "i=0\n",
    "for x,s,y in train_ds:\n",
    "    if s <=0:\n",
    "        neg.append(i)\n",
    "    i+=1\n",
    "\n",
    "neg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Neither'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.loc[118582,]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.drop(index = 118582, inplace=True)\n",
    "y_train.drop(index = 118582, inplace=True)\n",
    "X_train.reset_index(inplace=True, drop=True)\n",
    "y_train.reset_index(inplace=True, drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1000\n",
    "train_dl = DataLoader(train_ds, batch_size=batch_size, shuffle=True)\n",
    "valid_dl = DataLoader(valid_ds, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM-CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds =  YelpDataset(X_train, y_train, padding_start=False)\n",
    "valid_ds =  YelpDataset(X_valid, y_valid, padding_start=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 50\n",
    "train_dl = DataLoader(train_ds, batch_size=batch_size, shuffle=True)\n",
    "valid_dl = DataLoader(valid_ds, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "177681\n"
     ]
    }
   ],
   "source": [
    "V = vocab_size = len(words)\n",
    "print(V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "x,s,y = next(iter(train_dl))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([50, 400])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([50])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSTM\n",
    "embedding_dim = 300\n",
    "hidden_dim = 100\n",
    "\n",
    "embeddings = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n",
    "lstm = nn.LSTM(300, hidden_dim, batch_first=True)\n",
    "linear = nn.Linear(hidden_dim, 1)\n",
    "dropout = nn.Dropout(0.5)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1 = embeddings(x.long())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([50, 400, 300])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([50, 400, 300])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x2 = dropout(x1)\n",
    "x2.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([50, 100, 398])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv_3 = nn.Conv1d(in_channels=embedding_dim, out_channels=100, kernel_size=3)\n",
    "x3 = conv_3(x2.transpose(1,2))\n",
    "x3.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_4 = nn.Conv1d(in_channels=embedding_dim, out_channels=100, kernel_size=4)\n",
    "conv_5 = nn.Conv1d(in_channels=embedding_dim, out_channels=100, kernel_size=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([50, 100, 397]) torch.Size([50, 100, 396])\n"
     ]
    }
   ],
   "source": [
    "x4 = conv_4(x2.transpose(1,2))\n",
    "x5 = conv_5(x2.transpose(1,2))\n",
    "print(x4.size(), x5.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([50, 100, 1])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Combine\n",
    "# 100 3-gram detectors\n",
    "x3 = nn.ReLU()(x3)\n",
    "x3 = nn.MaxPool1d(kernel_size = 398)(x3)\n",
    "x3.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([50, 100, 1])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x4 = nn.ReLU()(x4)\n",
    "x4 = nn.MaxPool1d(kernel_size = 397)(x4)\n",
    "x4.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([50, 100, 1])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x5 = nn.ReLU()(x5)\n",
    "x5 = nn.MaxPool1d(kernel_size = 396)(x5)\n",
    "x5.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([50, 100, 3])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# concatenate x3, x4, x5\n",
    "out = torch.cat([x3, x4, x5], 1)\n",
    "out.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([50, 300])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out = out.view(out.size(0), -1)\n",
    "out.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_pack, (ht, ct) = lstm(out.unsqueeze(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([50, 1, 100])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out_pack.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 50, 100])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ht.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([50, 100])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ht[-1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# needs to convert x to (batch, hidden_dim, sentence_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([50, 1])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linear(ht[-1]).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN_LSTMModel(nn.Module):\n",
    "    \n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim):\n",
    "        super(CNN_LSTMModel, self).__init__()\n",
    "\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n",
    "        #LSTM layer\n",
    "        self.lstm = nn.LSTM(300, hidden_dim, batch_first=True)\n",
    "        #CNN layers\n",
    "        self.conv_3 = nn.Conv1d(in_channels=embedding_dim, out_channels=100, kernel_size=3)\n",
    "        self.conv_4 = nn.Conv1d(in_channels=embedding_dim, out_channels=100, kernel_size=4)\n",
    "        self.conv_5 = nn.Conv1d(in_channels=embedding_dim, out_channels=100, kernel_size=5)\n",
    "        self.dropout = nn.Dropout(p=0.1)\n",
    "        self.linear = nn.Linear(hidden_dim, 1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        x = self.dropout(x)\n",
    "        # CNN part\n",
    "        x = x.transpose(1,2)\n",
    "        x3 = F.relu(self.conv_3(x))\n",
    "        x4 = F.relu(self.conv_4(x))\n",
    "        x5 = F.relu(self.conv_5(x))\n",
    "        x3 = nn.MaxPool1d(kernel_size = 398)(x3)\n",
    "        x4 = nn.MaxPool1d(kernel_size = 397)(x4)\n",
    "        x5 = nn.MaxPool1d(kernel_size = 396)(x5)\n",
    "        out = torch.cat([x3, x4, x5], 1)\n",
    "        out = out.view(out.size(0), -1)\n",
    "         # LSTM part\n",
    "        out_pack, (ht, ct) = self.lstm(out.unsqueeze(1))\n",
    "        return self.linear(ht[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [],
   "source": [
    "def val_metrics(m, val_dl):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    sum_loss = 0.0\n",
    "    for x, s, y in val_dl:\n",
    "        x = x.long().cuda()\n",
    "        y = y.float().unsqueeze(1).cuda()\n",
    "        y_hat = model(x)\n",
    "        loss = F.binary_cross_entropy_with_logits(y_hat, y)\n",
    "        y_pred = y_hat > 0\n",
    "        #print(y_pred)\n",
    "        correct += (y_pred.float() == y).float().sum()\n",
    "        total += y.shape[0]\n",
    "        sum_loss += loss.item()*y.shape[0]\n",
    "    return sum_loss/total, correct/total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epocs(model, epochs=10, lr=0.001):\n",
    "    parameters = filter(lambda p: p.requires_grad, model.parameters())\n",
    "    optimizer = torch.optim.Adam(parameters, lr=lr)\n",
    "    for i in range(epochs):\n",
    "        model.train()\n",
    "        sum_loss = 0.0\n",
    "        total = 0\n",
    "        for x, s, y in train_dl:\n",
    "            x = x.long().cuda()\n",
    "            y = y.float().unsqueeze(1).cuda()\n",
    "            y_pred = model(x)\n",
    "            #print(y.size())\n",
    "            #print(y_pred.size())\n",
    "            optimizer.zero_grad()\n",
    "            loss = F.binary_cross_entropy_with_logits(y_pred, y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            sum_loss += loss.item()*y.shape[0]\n",
    "            total += y.shape[0]\n",
    "        val_loss, val_acc = val_metrics(model, val_dl)\n",
    "        print(\"train loss %.3f val loss %.3f and val accuracy %.3f\" % (sum_loss/total, val_loss, val_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(m, p): torch.save(m.state_dict(), p)\n",
    "    \n",
    "def load_model(m, p): m.load_state_dict(torch.load(p))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 500\n",
    "train_dl = DataLoader(train_ds, batch_size=batch_size, shuffle=True)\n",
    "val_dl = DataLoader(valid_ds, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 300\n",
    "hidden_dim = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "177681\n"
     ]
    }
   ],
   "source": [
    "vocab_size = len(words)\n",
    "print(vocab_size)\n",
    "\n",
    "model = CNN_LSTMModel(vocab_size, embedding_dim, hidden_dim).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss 0.704 val loss 0.693 and val accuracy 0.503\n",
      "train loss 0.698 val loss 0.699 and val accuracy 0.497\n",
      "train loss 0.700 val loss 0.709 and val accuracy 0.497\n",
      "train loss 0.700 val loss 0.701 and val accuracy 0.497\n",
      "train loss 0.699 val loss 0.693 and val accuracy 0.503\n"
     ]
    }
   ],
   "source": [
    "train_epocs(model, epochs=5, lr=0.1 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_model(model, '../model_300_100_5_0.1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss 0.864 val loss 0.694 and val accuracy 0.503\n",
      "train loss 0.729 val loss 0.795 and val accuracy 0.503\n",
      "train loss 0.713 val loss 0.704 and val accuracy 0.503\n",
      "train loss 0.736 val loss 1.310 and val accuracy 0.497\n",
      "train loss 0.737 val loss 0.694 and val accuracy 0.497\n"
     ]
    }
   ],
   "source": [
    "train_epocs(model, epochs=5, lr=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1 = CNN_LSTMModel(vocab_size, embedding_dim, hidden_dim).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss 0.695 val loss 0.696 and val accuracy 0.497\n",
      "train loss 0.694 val loss 0.696 and val accuracy 0.497\n",
      "train loss 0.694 val loss 0.696 and val accuracy 0.497\n",
      "train loss 0.694 val loss 0.696 and val accuracy 0.497\n",
      "train loss 0.694 val loss 0.696 and val accuracy 0.497\n"
     ]
    }
   ],
   "source": [
    "train_epocs(model1, epochs=5, lr=0.01 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_model(model1, '../model_300_100_5_0.01')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss 0.696 val loss 0.693 and val accuracy 0.503\n",
      "train loss 0.694 val loss 0.694 and val accuracy 0.497\n",
      "train loss 0.693 val loss 0.693 and val accuracy 0.503\n",
      "train loss 0.693 val loss 0.694 and val accuracy 0.503\n",
      "train loss 0.694 val loss 0.693 and val accuracy 0.503\n"
     ]
    }
   ],
   "source": [
    "model = CNN_LSTMModel(vocab_size, embedding_dim, hidden_dim).cuda()\n",
    "train_epocs(model, epochs=5, lr=0.01 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CNN_LSTMModel(vocab_size, embedding_dim, hidden_dim).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss 0.505 val loss 0.469 and val accuracy 0.784\n",
      "train loss 0.428 val loss 0.468 and val accuracy 0.765\n",
      "train loss 0.391 val loss 0.471 and val accuracy 0.784\n",
      "train loss 0.376 val loss 0.478 and val accuracy 0.769\n",
      "train loss 0.361 val loss 0.501 and val accuracy 0.779\n"
     ]
    }
   ],
   "source": [
    "train_epocs(model, epochs=5, lr=0.005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss 0.327 val loss 0.512 and val accuracy 0.778\n",
      "train loss 0.317 val loss 0.516 and val accuracy 0.774\n",
      "train loss 0.307 val loss 0.514 and val accuracy 0.776\n"
     ]
    }
   ],
   "source": [
    "train_epocs(model, epochs=3, lr=0.0005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss 0.298 val loss 0.520 and val accuracy 0.778\n",
      "train loss 0.296 val loss 0.524 and val accuracy 0.775\n",
      "train loss 0.294 val loss 0.524 and val accuracy 0.777\n"
     ]
    }
   ],
   "source": [
    "train_epocs(model, epochs=3, lr=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CNN_LSTMModel(vocab_size, embedding_dim, hidden_dim).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss 0.503 val loss 0.464 and val accuracy 0.785\n"
     ]
    }
   ],
   "source": [
    "train_epocs(model, epochs=1, lr=0.005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss 0.415 val loss 0.459 and val accuracy 0.789\n"
     ]
    }
   ],
   "source": [
    "train_epocs(model, epochs=1, lr=0.0005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss 0.398 val loss 0.458 and val accuracy 0.789\n"
     ]
    }
   ],
   "source": [
    "train_epocs(model, epochs=1, lr=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss 0.393 val loss 0.460 and val accuracy 0.790\n"
     ]
    }
   ],
   "source": [
    "train_epocs(model, epochs=1, lr=0.00005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss 0.391 val loss 0.459 and val accuracy 0.789\n"
     ]
    }
   ],
   "source": [
    "train_epocs(model, epochs=1, lr=0.00001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss 0.391 val loss 0.459 and val accuracy 0.789\n"
     ]
    }
   ],
   "source": [
    "train_epocs(model, epochs=1, lr=0.00005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss 0.389 val loss 0.460 and val accuracy 0.789\n",
      "train loss 0.386 val loss 0.459 and val accuracy 0.789\n"
     ]
    }
   ],
   "source": [
    "train_epocs(model, epochs=2, lr=0.00005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss 0.384 val loss 0.460 and val accuracy 0.789\n"
     ]
    }
   ],
   "source": [
    "train_epocs(model, epochs=1, lr=0.00001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss 0.383 val loss 0.460 and val accuracy 0.790\n"
     ]
    }
   ],
   "source": [
    "train_epocs(model, epochs=1, lr=0.00003)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_model(model, '../model_300_100_1_0.00003')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss 0.382 val loss 0.460 and val accuracy 0.790\n"
     ]
    }
   ],
   "source": [
    "train_epocs(model, epochs=1, lr=0.00003)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss 0.381 val loss 0.460 and val accuracy 0.790\n"
     ]
    }
   ],
   "source": [
    "train_epocs(model, epochs=1, lr=0.00003)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_model(model, '../model_300_100_1_0.00003')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
